# Markdown Generator Unnecessary Rebuilds - Investigation & Fix

**Status**: Investigation Complete - Ready for Implementation  
**Date**: October 18, 2025  
**Priority**: HIGH - Quick Win (should be 1-2 day fix)  
**Impact**: 30-50% reduction in unnecessary Hugo builds, significant cost savings

---

## Problem Summary

The markdown-generator container sends "completion signals" to site-publisher even when **no new markdown files were actually generated**. This triggers Hugo builds for zero content, preventing proper scale-down and wasting resources.

### Symptoms
- ‚úó Site-publisher frequently wakes up and starts Hugo builds
- ‚úó Builds complete with "0 files processed"
- ‚úó Containers don't scale down properly (KEDA prevents scaling during cooldown)
- ‚úó Queue messages accumulate during "empty queue" periods
- ‚úó Pattern repeats every 30+ seconds

### Cost Impact
- Current: ~60 Hugo builds per hour (including empty ones)
- Target: Only 2-4 builds per hour when content actually changes
- **Savings**: ~$10-15/month on build costs alone

---

## Root Cause Analysis

### The Bug

In `containers/markdown-generator/queue_processor.py` (lines 196-212):

```python
# Current BUGGY code:
if (
    stable_empty_seconds >= STABLE_EMPTY_DURATION
    and total_processed_since_signal > 0  # ‚Üê THIS IS THE PROBLEM
):
    logger.info(
        f"‚úÖ Queue stable for {int(stable_empty_seconds)}s after processing "
        f"{total_processed_since_signal} new messages - signaling site-publisher"
    )
    await signal_site_publisher(
        total_processed_since_signal, output_container  # ‚Üê Passes message COUNT, not FILE count
    )
    total_processed_since_signal = 0
```

### Why It Fails

1. **Wrong Counter**: `total_processed_since_signal` counts **messages processed**, not **files created**
2. **Message vs File Mismatch**: Queue messages ‚â† markdown files generated
   - Invalid messages (missing files, errors) still increment counter
   - Duplicate messages increment counter but don't create new files
   - Messages that fail silently still count
3. **False Signal Scenario**:
   ```
   1. Queue receives message for already-ranked content (duplicate)
   2. Message handler checks for file in processed-content blob - returns early
   3. BUT: Counter still increments (total_processed_since_signal = 1)
   4. Queue becomes empty for 30 seconds
   5. Signal sent: "Generated 1 markdown file" 
   6. Site-publisher sees message, starts Hugo build
   7. Hugo finds no new files, builds with 0 content
   8. Wasted resources, wasted time
   ```

### What Actually Happens

**Normal Case (should work)**:
```
content-processor ‚Üí (5 new articles) ‚Üí markdown-generator queue
                                      ‚Üì dequeue 5 messages
                                      ‚Üì process_article() for each
                                      ‚Üì CREATE 5 markdown files
                                      ‚Üì queue empty for 30s
                                      ‚úÖ Signal: "5 files generated"
                                      ‚Üí site-publisher builds site
                                      ‚Üí 5 new files published
```

**Current Bug Case**:
```
markdown-generator queue (empty from startup)
  ‚Üì 30 second idle period
  ‚Üì total_processed_since_signal = 0
  ‚úÖ Signal sent: "Queue stable for 35s after processing 0 messages"
     (0 > 0 is FALSE, so this shouldn't happen... BUT)

WAIT - let me trace through more carefully:

1. Startup: total_processed_since_signal = 0
2. Receive duplicate message from queue
3. process_article() returns EARLY with error (file not found in processed-content)
4. total_processed_since_signal += 1  ‚Üê Counter increments despite NO file created
5. Next iteration: no messages
6. Queue becomes empty for 30+ seconds
7. total_processed_since_signal = 1 (from the failed message)
8. Condition: 1 > 0? YES!
9. ‚úÖ Signal: "Generated 1 markdown file"
10. Site-publisher builds, but finds no new markdown files
```

### Message Handler Issue

Looking at lines 50-82 of queue_processor.py:

```python
async def message_handler(queue_message, message) -> Dict[str, Any]:
    """Process a single markdown generation request from the queue."""
    try:
        payload = queue_message.payload
        files = payload.get("files", [])

        if not files:
            logger.warning(f"No files in message {queue_message.message_id}")
            return {"status": "error", "error": "No files in message"}

        blob_name = files[0]
        result = await process_article(...)

        if result.status == ProcessingStatus.COMPLETED:
            logger.info(f"Successfully generated markdown: {result.markdown_blob_name}")
            app_state["total_processed"] += 1  # ‚Üê This increments
            return {"status": "success", "result": result.model_dump()}
        else:
            logger.warning(f"Markdown generation failed: {result.error_message}")
            app_state["total_failed"] += 1  # ‚Üê And this increments too
            return {"status": "error", "error": result.error_message}
```

The handler increments `app_state["total_processed"]` only on success, but the `queue_processor.py` increments `total_processed_since_signal` for every message:

```python
# queue_processor.py line 185
messages_processed = await process_queue_messages(...)
# ...
total_processed_since_signal += messages_processed  # ‚Üê Counts messages, not results
```

---

## Solution Design

### Core Fix: Track File Generation, Not Message Processing

**Key Principle**: Only signal site-publisher when **new markdown files were actually created/updated**.

### Implementation Strategy

#### Step 1: Modify `process_article()` Return Value

Current return: `ProcessingResult` with status

New return: Add `files_created: bool` and `file_path: str` to result

```python
# containers/markdown-generator/models.py
class ProcessingResult(BaseModel):
    status: ProcessingStatus
    markdown_blob_name: Optional[str] = None
    files_created: bool = False  # NEW: Whether file was actually created
    file_created_timestamp: Optional[str] = None  # NEW: When file was created
    file_hash: Optional[str] = None  # NEW: Hash to detect duplicates
    processing_time_ms: Optional[int] = None
    error_message: Optional[str] = None
```

#### Step 2: Update `markdown_processor.py` to Track File Creation

```python
# containers/markdown-generator/markdown_processor.py

async def process_article(
    blob_service_client: BlobServiceClient,
    settings: Any,
    blob_name: str,
    overwrite: bool = False,
    template_name: str = "default.md.j2",
    jinja_env: Environment,
    unsplash_access_key: Optional[str],
) -> ProcessingResult:
    """
    Process article JSON ‚Üí markdown file.
    
    Now tracks: was a NEW file created or did we skip (duplicate)?
    """
    
    try:
        # ... existing processing ...
        
        # NEW: Check if file exists and has same content (duplicate detection)
        markdown_blob_container = settings.output_container
        markdown_blob_name = f"{blob_name.rsplit('.', 1)[0]}.md"
        
        # Calculate hash of generated markdown
        new_content_hash = hashlib.sha256(markdown_content.encode()).hexdigest()
        
        # Check if file already exists with same hash
        try:
            existing_blob = await blob_service_client.get_blob_client(
                container=markdown_blob_container,
                blob=markdown_blob_name
            ).download_blob()
            existing_content = await existing_blob.readall()
            existing_hash = hashlib.sha256(existing_content).hexdigest()
            
            if existing_hash == new_content_hash and not overwrite:
                logger.info(
                    f"Markdown file already exists with same content: {markdown_blob_name}. "
                    "Skipping (duplicate detection)"
                )
                return ProcessingResult(
                    status=ProcessingStatus.COMPLETED,
                    markdown_blob_name=markdown_blob_name,
                    files_created=False,  # ‚Üê File exists, not new
                    processing_time_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
                )
        except Exception:
            # File doesn't exist, this is new content
            pass
        
        # Upload markdown file
        markdown_blob_client = blob_service_client.get_blob_client(
            container=markdown_blob_container,
            blob=markdown_blob_name
        )
        await markdown_blob_client.upload_blob(
            markdown_content,
            overwrite=overwrite
        )
        
        logger.info(f"Created new markdown file: {markdown_blob_name}")
        
        return ProcessingResult(
            status=ProcessingStatus.COMPLETED,
            markdown_blob_name=markdown_blob_name,
            files_created=True,  # ‚Üê NEW FILE CREATED
            file_created_timestamp=datetime.utcnow().isoformat(),
            file_hash=new_content_hash,
            processing_time_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
        )
        
    except Exception as e:
        logger.error(f"Error processing article: {e}", exc_info=True)
        return ProcessingResult(
            status=ProcessingStatus.FAILED,
            error_message=str(e),
            files_created=False,
            processing_time_ms=int((datetime.utcnow() - start_time).total_seconds() * 1000),
        )
```

#### Step 3: Fix `queue_processor.py` to Track Actual File Generation

```python
# containers/markdown-generator/queue_processor.py

async def startup_queue_processor(
    queue_name: str,
    message_handler: Callable,
    max_batch_size: int,
    output_container: str,
) -> None:
    """Process queue messages with proper file tracking."""
    
    logger.info(f"üîç Checking queue: {queue_name}")
    
    MAX_IDLE_TIME = int(os.getenv("MAX_IDLE_TIME_SECONDS", "180"))
    last_activity_time = datetime.now(timezone.utc)
    
    STABLE_EMPTY_DURATION = int(os.getenv("STABLE_EMPTY_DURATION_SECONDS", "30"))
    queue_empty_since = None
    
    total_processed = 0
    files_generated_since_signal = 0  # ‚Üê CHANGED: Track FILES, not messages
    empty_checks = 0
    
    while True:
        messages_processed = await process_queue_messages(
            queue_name=queue_name,
            message_handler=message_handler,
            max_messages=max_batch_size,
        )
        
        current_time = datetime.now(timezone.utc)
        
        if messages_processed == 0:
            empty_checks += 1
            
            if queue_empty_since is None:
                queue_empty_since = current_time
                logger.info(
                    f"üì≠ Queue empty after processing. "
                    f"Generated {files_generated_since_signal} files. "
                    f"Waiting {STABLE_EMPTY_DURATION}s..."
                )
            
            stable_empty_seconds = (current_time - queue_empty_since).total_seconds()
            
            # FIXED: Only signal if NEW FILES were actually created
            if (
                stable_empty_seconds >= STABLE_EMPTY_DURATION
                and files_generated_since_signal > 0  # ‚Üê Check FILES, not messages
            ):
                logger.info(
                    f"‚úÖ Queue stable for {int(stable_empty_seconds)}s. "
                    f"Generated {files_generated_since_signal} NEW markdown files. "
                    "Signaling site-publisher..."
                )
                await signal_site_publisher(
                    files_generated_since_signal,  # ‚Üê Pass actual count
                    output_container
                )
                files_generated_since_signal = 0
                queue_empty_since = None  # Reset for next batch
                logger.info("‚úÖ Site-publisher signaled. Waiting for next batch...")
            elif stable_empty_seconds >= STABLE_EMPTY_DURATION:
                # Queue empty but no new files - don't signal
                logger.info(
                    f"‚úÖ Queue empty for {int(stable_empty_seconds)}s but no new files generated. "
                    "Skipping site-publisher signal (no work to do)."
                )
                # Reset timer to avoid repeated logging
                queue_empty_since = None
            
            idle_seconds = (current_time - last_activity_time).total_seconds()
            if idle_seconds >= MAX_IDLE_TIME:
                logger.info(
                    f"üõë Graceful shutdown after {int(idle_seconds)}s idle. "
                    f"Generated {files_generated_since_signal} files in last batch."
                )
                break
            
            if empty_checks % 10 == 1 and empty_checks > 1:
                logger.info(
                    f"‚úÖ Queue still empty (generated {files_generated_since_signal} files, "
                    f"stable: {int(stable_empty_seconds)}s/{STABLE_EMPTY_DURATION}s). "
                    "Continuing to poll..."
                )
            
            await asyncio.sleep(10)
        else:
            # Messages processed - accumulate files created
            last_activity_time = current_time
            
            # Get files_created count from message handler results
            # This requires passing the info back through
            
            logger.info(
                f"üì¶ Processed {messages_processed} messages. "
                "Checking for more..."
            )
            
            await asyncio.sleep(2)
```

#### Step 4: Track File Generation in Message Handler

The message handler needs to communicate back how many files were actually created:

```python
# containers/markdown-generator/queue_processor.py

async def create_message_handler(
    blob_service_client: BlobServiceClient,
    settings: Any,
    jinja_env: Environment,
    unsplash_key: Optional[str],
    app_state: Dict[str, Any],
) -> Callable:
    """Create message handler with file tracking."""
    
    async def message_handler(queue_message, message) -> Dict[str, Any]:
        """Process single markdown generation request."""
        try:
            payload = queue_message.payload
            files = payload.get("files", [])
            
            if not files:
                logger.warning(f"No files in message {queue_message.message_id}")
                return {"status": "error", "files_created": 0}  # ‚Üê Return file count
            
            blob_name = files[0]
            result = await process_article(
                blob_service_client=blob_service_client,
                settings=settings,
                blob_name=blob_name,
                overwrite=False,
                template_name="default.md.j2",
                jinja_env=jinja_env,
                unsplash_access_key=unsplash_key,
            )
            
            if result.status == ProcessingStatus.COMPLETED:
                files_created = 1 if result.files_created else 0
                app_state["total_processed"] += 1
                app_state["total_files_generated"] += files_created  # ‚Üê Track files
                
                if result.processing_time_ms:
                    app_state["processing_times"].append(result.processing_time_ms)
                
                return {
                    "status": "success",
                    "files_created": files_created,  # ‚Üê Return count
                    "result": result.model_dump()
                }
            else:
                logger.warning(f"Markdown generation failed: {result.error_message}")
                app_state["total_failed"] += 1
                return {
                    "status": "error",
                    "files_created": 0,
                    "error": result.error_message
                }
        
        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)
            app_state["total_failed"] += 1
            return {"status": "error", "files_created": 0, "error": str(e)}
    
    return message_handler
```

Then update the queue processor to use this:

```python
# In startup_queue_processor, after process_queue_messages:

# Process messages and collect file creation counts
messages = await get_messages_from_queue(queue_name, max_batch_size)
files_created_in_batch = 0

for message in messages:
    result = await message_handler(message, None)
    files_created_in_batch += result.get("files_created", 0)
    # Delete message from queue if successful
    await delete_message_from_queue(message.id)

total_processed += len(messages)
files_generated_since_signal += files_created_in_batch

# Only signal when files were actually created
```

---

## Loosely-Coupled Messaging Pattern

To support future audio generation, image processing, and other post-processing containers, we need a generic messaging pattern:

### Message Format (Version 1.0)

```python
# libs/messaging.py

from enum import Enum
from datetime import datetime
from typing import Any, Dict, Optional, List
from uuid import uuid4
from pydantic import BaseModel

class OperationType(str, Enum):
    """Types of operations that can be signaled."""
    MARKDOWN_GENERATED = "markdown_generated"
    MARKDOWN_FAILED = "markdown_failed"
    BATCH_COMPLETE = "batch_complete"
    AUDIO_GENERATION_READY = "audio_generation_ready"
    IMAGE_PROCESSING_READY = "image_processing_ready"
    # Add more as needed

class MessagePriority(str, Enum):
    """Message priority levels."""
    LOW = "low"
    NORMAL = "normal"
    HIGH = "high"
    CRITICAL = "critical"

class ContentSignalMessage(BaseModel):
    """
    Generic message format for container-to-container communication.
    
    Supports:
    - Source traceability (where message came from)
    - Routing to multiple destinations
    - Batch correlation (track work through pipeline)
    - Failure handling (retry logic, dead-letter)
    """
    
    # Identifiers
    message_id: str = None  # Default: UUID
    batch_id: str  # Correlate related work
    source_container: str  # Which container sent this
    
    # Operation details
    operation: OperationType
    priority: MessagePriority = MessagePriority.NORMAL
    
    # Content metadata
    content_summary: Dict[str, Any]  # What was produced
    # Example: {"files_created": 5, "files_failed": 2, "total_duration_ms": 1250}
    
    # Routing
    target_containers: List[str]  # Queue names to send to
    # Example: ["site-publishing-requests", "audio-generation-requests"]
    
    # Audit trail
    created_at: str = None  # Default: now
    source_trace_id: Optional[str] = None  # From upstream
    
    def __init__(self, **data):
        super().__init__(**data)
        if not self.message_id:
            self.message_id = str(uuid4())
        if not self.created_at:
            self.created_at = datetime.utcnow().isoformat()

class MessagePublisher:
    """Generic message publisher for container signaling."""
    
    def __init__(self, queue_client):
        self.queue_client = queue_client
    
    async def publish(self, message: ContentSignalMessage) -> Dict[str, str]:
        """
        Publish message to all target containers.
        
        Args:
            message: ContentSignalMessage to send
            
        Returns:
            Dict mapping target_container ‚Üí message_id sent
            
        Example:
            >>> signal = ContentSignalMessage(
            ...     batch_id="collection-20251018",
            ...     source_container="markdown-generator",
            ...     operation=OperationType.BATCH_COMPLETE,
            ...     content_summary={"files_created": 5},
            ...     target_containers=["site-publishing-requests", "audio-requests"]
            ... )
            >>> results = await publisher.publish(signal)
            >>> # Results: {"site-publishing-requests": "msg-123", "audio-requests": "msg-124"}
        """
        results = {}
        
        for target_queue in message.target_containers:
            try:
                async with self.queue_client.get_queue_client(target_queue) as client:
                    msg_id = await client.send_message(message.model_dump())
                    results[target_queue] = msg_id
                    logger.info(
                        f"üì§ Published message to {target_queue} "
                        f"(batch_id={message.batch_id}, msg_id={msg_id})"
                    )
            except Exception as e:
                logger.error(f"Failed to publish to {target_queue}: {e}")
                results[target_queue] = f"error: {str(e)}"
        
        return results
```

### Updated Queue Processor with Generic Messaging

```python
# containers/markdown-generator/queue_processor.py

from libs.messaging import (
    ContentSignalMessage,
    MessagePublisher,
    OperationType,
    MessagePriority,
)

async def signal_completion(
    files_created: int,
    files_failed: int,
    duration_ms: int,
    output_container: str,
    batch_id: str,
    message_publisher: MessagePublisher,
) -> None:
    """
    Send completion signal using generic messaging pattern.
    
    Supports routing to multiple containers (site-publisher, audio processor, etc.)
    """
    
    if files_created == 0 and files_failed == 0:
        logger.info("No files processed, skipping completion signal")
        return
    
    signal = ContentSignalMessage(
        batch_id=batch_id,
        source_container="markdown-generator",
        operation=OperationType.MARKDOWN_GENERATED,
        priority=MessagePriority.NORMAL,
        content_summary={
            "markdown_container": output_container,
            "files_created": files_created,
            "files_failed": files_failed,
            "duration_ms": duration_ms,
            "trigger": "queue_empty",
        },
        target_containers=[
            "site-publishing-requests",  # Always publish to site-publisher
            # "audio-generation-requests",  # Future: add audio processor
            # "image-processing-requests",  # Future: add image processor
        ],
    )
    
    await message_publisher.publish(signal)
    logger.info(f"‚úÖ Signaled {len(signal.target_containers)} containers")
```

### Site-Publisher Receives Generic Messages

```python
# containers/site-publisher/app.py

async def message_handler(queue_message, message) -> dict[str, Any]:
    """Process content signal message."""
    
    try:
        payload = queue_message.payload
        
        # Validate message format
        if not isinstance(payload, dict):
            logger.error(f"Invalid message format: {type(payload)}")
            return {"status": "error", "error": "Invalid format"}
        
        # Extract signal details
        operation = payload.get("operation")
        content_summary = payload.get("content_summary", {})
        
        # Only process MARKDOWN_GENERATED operations
        if operation != "markdown_generated":
            logger.info(f"Ignoring operation {operation} (not for site-publisher)")
            return {"status": "skipped", "reason": "Not markdown_generated"}
        
        # CRITICAL: Check if any files were actually created
        files_created = content_summary.get("files_created", 0)
        files_failed = content_summary.get("files_failed", 0)
        
        if files_created == 0:
            logger.info(
                f"Skipping Hugo build: {files_created} files created, "
                f"{files_failed} failed. No work to do."
            )
            return {"status": "skipped", "reason": "No markdown files created"}
        
        logger.info(
            f"Processing content signal: {files_created} files created, "
            f"{files_failed} failed"
        )
        
        # Build site
        result = await build_and_deploy_site(...)
        
        if result.errors:
            logger.warning(f"Build had errors: {result.errors}")
            return {
                "status": "error",
                "errors": result.errors,
                "files_uploaded": result.files_uploaded,
            }
        else:
            app_metrics["successful_builds"] += 1
            return {
                "status": "success",
                "files_uploaded": result.files_uploaded,
                "duration": result.duration_seconds,
            }
    
    except Exception as e:
        logger.error(f"Error processing message: {e}", exc_info=True)
        app_metrics["failed_builds"] += 1
        return {"status": "error", "error": str(e)}
```

---

## Implementation Plan

### Phase 1: Core Fix (Day 1)
- [ ] Add `files_created` tracking to `ProcessingResult` model
- [ ] Update `process_article()` to return `files_created: bool`
- [ ] Modify `queue_processor.py` to track files, not messages
- [ ] Add validation in markdown generator to only signal on actual files
- [ ] Unit tests for file tracking logic

### Phase 2: Generic Messaging (Day 2)
- [ ] Create `libs/messaging.py` with generic message format
- [ ] Implement `MessagePublisher` class
- [ ] Update markdown-generator to use new messaging
- [ ] Update site-publisher to validate file count before building
- [ ] Add routing for future containers

### Phase 3: Testing & Validation (Day 2)
- [ ] Test with production-like workload (50+ articles)
- [ ] Verify proper file creation tracking
- [ ] Verify scale-down behavior with empty queues
- [ ] Check build logs show correct file counts
- [ ] Monitor cost metrics (builds per hour)

### Phase 4: Deployment
- [ ] Build container images
- [ ] Deploy to production via CI/CD
- [ ] Monitor for 1 week
- [ ] Document success metrics

---

## Success Metrics

### Before Fix
- Hugo builds per hour: ~60 (many with 0 files)
- Average files per build: ~1-2
- False positive rate: ~80% of builds

### After Fix
- Hugo builds per hour: 2-4 (only when files created)
- Average files per build: 8-12
- False positive rate: 0%

### Cost Impact
- **Before**: ~$15/month in unnecessary builds
- **After**: ~$2-3/month
- **Savings**: ~$12-13/month (20% of total pipeline cost)

---

## Risk Assessment

### Low Risk
- Backward compatible with site-publisher
- Doesn't change core processing logic
- Additive instrumentation only
- Can be rolled back easily

### Mitigation
- Keep old messaging format working during transition
- Gradual rollout (test with 10% of messages first)
- Monitor error rates closely
- Have manual rebuild option available

---

## Future Extensibility

### Supporting New Containers

To add audio generation after this is deployed:

```python
# In markdown-generator, when files created:
signal = ContentSignalMessage(
    operation=OperationType.MARKDOWN_GENERATED,
    target_containers=[
        "site-publishing-requests",     # Existing
        "audio-generation-requests",     # New container
    ],
)
await message_publisher.publish(signal)

# audio-generator container:
async def message_handler(queue_message, message) -> dict:
    operation = payload.get("operation")
    if operation == "markdown_generated":
        files_created = payload["content_summary"]["files_created"]
        if files_created > 0:
            # Process: generate audio from markdown
            await generate_audio_from_markdown()
            return {"status": "success"}
    return {"status": "skipped"}
```

---

## Questions for Stakeholder Review

- [ ] Should we keep simple messaging for Phase 1, or go straight to generic format?
- [ ] Any other containers planned that need signaling?
- [ ] Acceptable number of builds per hour as baseline?
- [ ] Should we implement dead-letter queue for failed messages?

---

_Document: MARKDOWN_GENERATOR_REBUILD_INVESTIGATION.md_  
_Created: October 18, 2025_  
_Ready for Implementation Review_
