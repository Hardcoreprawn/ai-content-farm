#!/usr/bin/env python3
"""
Test script to validate the wake-up message processing pattern with Storage Queues.

This script tests the updated architecture where:
1. Collector sends a single wake-up message per collection batch to Storage Queue
2. Processor receives wake-up message from Storage Queue and scans all available work in blob storage
3. KEDA scales from 0‚Üí1 on any message arrival using azure-queue scaler with managed identity
4. Processing happens for all collected content in storage
5. Storage Queues resolve authentication conflicts with Container Apps managed identity

Usage:
    python test_wake_up_pattern.py
"""

import asyncio
import json
import logging
import sys
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Any, Dict

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class WakeUpPatternValidator:
    """Validates the wake-up message processing architecture."""

    def __init__(self):
        self.test_results = {
            "collector_sends_wake_up": False,
            "processor_handles_wake_up": False,
            "storage_queue_scaling_responsive": False,
            "end_to_end_flow_working": False,
        }

    async def validate_collector_wake_up_logic(self) -> bool:
        """Validate that the collector sends wake-up messages."""
        try:
            logger.info("üîç Validating collector wake-up message logic...")

            # Test that ContentCollectorService has wake-up message logic
            sys.path.insert(
                0, "/workspaces/ai-content-farm/containers/content-collector"
            )
            sys.path.insert(0, "/workspaces/ai-content-farm")
            from service_logic import ContentCollectorService

            # Create a test service instance
            service = ContentCollectorService()

            # Check that _send_processing_request exists and sends wake-up messages
            import inspect

            method = getattr(service, "_send_processing_request", None)
            if method:
                signature = inspect.signature(method)
                logger.info(
                    f"‚úì _send_processing_request method found: {signature}")

                # Check if it's the wake-up pattern (collection_result parameter, not individual items)
                params = list(signature.parameters.keys())
                if "collection_result" in params:
                    logger.info(
                        "‚úì Wake-up pattern detected (collection_result parameter)"
                    )
                    self.test_results["collector_sends_wake_up"] = True
                    return True
                elif "items" in params:
                    logger.warning(
                        f"‚ö†Ô∏è Method signature suggests individual items: {params}"
                    )
                    return False
                else:
                    logger.warning(f"‚ö†Ô∏è Unexpected method signature: {params}")
                    return False
            else:
                logger.error("‚úó _send_processing_request method not found")
                return False

        except Exception as e:
            logger.error(f"‚úó Collector validation failed: {e}")
            return False

    async def validate_processor_wake_up_logic(self) -> bool:
        """Validate that the processor handles wake-up messages correctly with Storage Queues."""
        try:
            logger.info(
                "üîç Validating processor wake-up message handling with Storage Queues...")

            # Check source code for wake-up pattern instead of importing
            processor_router_path = Path(
                "/workspaces/ai-content-farm/containers/content-processor/endpoints/storage_queue_router.py"
            )

            # Fallback to servicebus_router.py if storage_queue_router.py doesn't exist yet
            if not processor_router_path.exists():
                processor_router_path = Path(
                    "/workspaces/ai-content-farm/containers/content-processor/endpoints/servicebus_router.py"
                )
                logger.warning(
                    "‚ö†Ô∏è Storage Queue router not found, checking Service Bus router for migration")

            if not processor_router_path.exists():
                logger.error("‚úó No processor router file found")
                return False

            source_code = processor_router_path.read_text()

            # Check for wake-up processing patterns in source
            wake_up_indicators = [
                "wake_up",  # Operation type
                "process_available_work",  # Method that processes all work
                "process_message_payload",  # Message handling method
            ]

            found_indicators = []
            for indicator in wake_up_indicators:
                if indicator in source_code:
                    found_indicators.append(indicator)
                    logger.info(f"‚úì Found wake-up indicator: {indicator}")
                else:
                    logger.warning(
                        f"‚ö†Ô∏è Missing wake-up indicator: {indicator}")

            if len(found_indicators) >= 2:  # At least 2/3 indicators
                logger.info("‚úì Wake-up processing logic detected in processor")
                self.test_results["processor_handles_wake_up"] = True
                return True
            else:
                logger.error(
                    f"‚úó Insufficient wake-up indicators ({len(found_indicators)}/4)"
                )
                return False

        except Exception as e:
            logger.error(f"‚úó Processor validation failed: {e}")
            return False

    async def validate_keda_configuration(self) -> bool:
        """Validate KEDA scaling configuration for Storage Queue responsiveness."""
        try:
            logger.info(
                "üîç Validating Storage Queue KEDA scaling configuration...")

            # Check Terraform configuration for KEDA scaling rules
            container_apps_tf = Path(
                "/workspaces/ai-content-farm/infra/container_apps.tf"
            )
            if not container_apps_tf.exists():
                logger.error("‚úó container_apps.tf not found")
                return False

            tf_content = container_apps_tf.read_text()

            # Look for Storage Queue KEDA scaling configuration
            checks = [
                ('custom_rule_type = "azure-queue"', "Azure Queue scaler type"),
                ('queueLength = "1"', "Queue length = 1 for immediate scaling"),
                ("custom_scale_rule", "KEDA custom scaling rules"),
                ("accountName", "Storage account name in metadata"),
                ("queueName", "Queue name in metadata"),
            ]

            passed = 0
            for check_str, description in checks:
                if check_str in tf_content:
                    logger.info(
                        f"‚úì Found Storage Queue scaling config: {description}")
                    passed += 1
                else:
                    logger.warning(
                        f"‚ö†Ô∏è Missing Storage Queue config: {description}")

            if passed >= 3:  # At least scaler type, queue length, and scale rule
                logger.info(
                    "‚úì KEDA configured for Storage Queue scaling with immediate responsiveness"
                )
                self.test_results["storage_queue_scaling_responsive"] = True
                return True
            else:
                logger.error(
                    "‚úó KEDA not configured for Storage Queue responsive scaling")
                return False

        except Exception as e:
            logger.error(f"‚úó KEDA configuration validation failed: {e}")
            return False

    async def validate_integration_points(self) -> bool:
        """Validate that all integration points are properly configured."""
        try:
            logger.info("üîç Validating integration points...")

            checks = []

            # Check Storage Queue configuration
            try:
                sys.path.insert(0, "/workspaces/ai-content-farm")
                from libs.storage_queue_client import StorageQueueClient

                logger.info("‚úì Storage Queue client library available")
                checks.append(True)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Storage Queue client library issue (expected during migration): {e}")
                # Try Service Bus as fallback during migration
                try:
                    from libs.service_bus_client import ServiceBusClient
                    logger.info("‚úì Service Bus client library available (fallback during migration)")
                    checks.append(True)
                except Exception as e2:
                    logger.error(f"‚úó No queue client library available: {e2}")
                    checks.append(False)

            # Check blob storage integration
            try:
                from libs.blob_storage import BlobStorageClient

                logger.info("‚úì Blob storage client library available")
                checks.append(True)
            except Exception as e:
                logger.error(f"‚úó Blob storage client library issue: {e}")
                checks.append(False)

            # Check processor core logic
            try:
                processor_path = Path(
                    "/workspaces/ai-content-farm/containers/content-processor/processor.py"
                )
                if processor_path.exists():
                    logger.info("‚úì Content processor core logic file exists")
                    checks.append(True)
                else:
                    logger.error("‚úó Content processor core logic file missing")
                    checks.append(False)
            except Exception as e:
                logger.error(f"‚úó Content processor core logic issue: {e}")
                checks.append(False)

            success_rate = sum(checks) / len(checks)
            if success_rate >= 0.75:  # 75% of integrations working
                logger.info(
                    f"‚úì Integration points mostly working ({success_rate:.1%})")
                self.test_results["end_to_end_flow_working"] = True
                return True
            else:
                logger.error(
                    f"‚úó Too many integration issues ({success_rate:.1%} working)"
                )
                return False

        except Exception as e:
            logger.error(f"‚úó Integration validation failed: {e}")
            return False
                from libs.storage_queue_client import StorageQueueClient

                logger.info("‚úì Storage Queue client library available")
                checks.append(True)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Storage Queue client library issue (expected during migration): {e}")
                # Try Service Bus as fallback during migration
                try:
                    from libs.service_bus_client import ServiceBusClient
                    logger.info("‚úì Service Bus client library available (fallback during migration)")
                    checks.append(True)
                except Exception as e2:
                    logger.error(f"‚úó No queue client library available: {e2}")
                    checks.append(False)

            # Check blob storage integration
            try:
                from libs.blob_storage import BlobStorageClient

                logger.info("‚úì Blob storage client library available")
                checks.append(True)
            except Exception as e:
                logger.error(f"‚úó Blob storage client library issue: {e}")
                checks.append(False)

            # Check processor core logic
            try:
                processor_path = Path(
                    "/workspaces/ai-content-farm/containers/content-processor/processor.py"
                )
                if processor_path.exists():
                    logger.info("‚úì Content processor core logic file exists")
                    checks.append(True)
                else:
                    logger.error("‚úó Content processor core logic file missing")
                    checks.append(False)
            except Exception as e:
                logger.error(f"‚úó Content processor core logic issue: {e}")
                checks.append(False)

            success_rate = sum(checks) / len(checks)
            if success_rate >= 0.75:  # 75% of integrations working
                logger.info(
                    f"‚úì Integration points mostly working ({success_rate:.1%})")
                self.test_results["end_to_end_flow_working"] = True
                return True
            else:
                logger.error(
                    f"‚úó Too many integration issues ({success_rate:.1%} working)"
                )
                return False

        except Exception as e:
            logger.error(f"‚úó Integration validation failed: {e}")
            return False

    async def run_validation(self) -> Dict[str, Any]:
        """Run all validations and return results."""
        logger.info(
            "üîç Starting validation of wake-up message processing architecture..."
        )
        logger.info("")

        # Run all validation tests
        validations = [
            ("Collector Sends Wake-Up Messages",
             self.validate_collector_wake_up_logic),
            (
                "Processor Handles Wake-Up Messages",
                self.validate_processor_wake_up_logic,
            ),
            ("Storage Queue KEDA Scaling Responsive", self.validate_keda_configuration),
            ("Integration Points Working", self.validate_integration_points),
        ]

        results = {}
        for name, validation_func in validations:
            try:
                result = await validation_func()
                results[name] = "PASS" if result else "FAIL"
            except Exception as e:
                logger.error(f"‚úó {name} validation crashed: {e}")
                results[name] = "FAIL"

        return results

    def print_summary(self, results: Dict[str, Any]):
        """Print validation summary."""
        logger.info("")
        logger.info("üìä WAKE-UP PATTERN VALIDATION SUMMARY")
        logger.info("==================================================")

        passed = sum(1 for result in results.values() if result == "PASS")
        total = len(results)
        logger.info(f"‚úÖ Tests Passed: {passed}/{total}")
        logger.info("")

        for test_name, result in results.items():
            status = "‚úÖ PASS" if result == "PASS" else "‚ùå FAIL"
            logger.info(f"  {status}: {test_name}")

        logger.info("")

        if passed == total:
            logger.info(
                "üéâ All validations passed! Wake-up pattern is working correctly."
            )
            return True
        elif passed >= total * 0.75:
            logger.info(
                "‚ö†Ô∏è  Most validations passed - minor issues to address")
            return True
        else:
            logger.info(
                "‚ö†Ô∏è  Some validations failed - review above for details")
            return False


async def main():
    """Main test execution."""
    validator = WakeUpPatternValidator()

    try:
        results = await validator.run_validation()
        success = validator.print_summary(results)
        sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        logger.info("\nüõë Validation cancelled by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"üí• Validation crashed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
