name: Individual Large File Issue Creator

on:
  push:
    branches: [main, develop]
    paths-ignore:
      - 'docs/**'
      - '**.md'
      - '.github/workflows/**'
  pull_request:
    branches: [main, develop]
    paths-ignore:
      - 'docs/**'
      - '**.md'
      - '.github/workflows/**'
  workflow_dispatch:  # Allow manual triggering
  schedule:
    - cron: '0 6 * * 1'  # Weekly on Monday at 6 AM UTC

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  detect-large-files:
    name: Create Individual Issues for Large Files
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Find Large Files
        id: find-large
        shell: bash
        run: |
          echo "Scanning for files over 500 lines..."

          # Get list of changed files (for PR) or all files (for push to main)
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            # For PRs, only check changed files
            changed_files=$(git diff --name-only origin/${{ github.base_ref }}...HEAD)
          else
            # For pushes to main, check all tracked files
            changed_files=$(git ls-files)
          fi

          large_files=""
          file_count=0

          # Check each file
          while IFS= read -r file; do
            if [[ -z "$file" ]]; then continue; fi

            # Skip binary files, images, and other non-source files
            if [[ "$file" =~ \.(jpg|jpeg|png|gif|ico|pdf|zip|tar|gz|bz2|xz|7z|rar|exe|dll|so|dylib|bin|dat)$ ]]; then
              continue
            fi

            # Skip directories
            if [[ ! -f "$file" ]]; then
              continue
            fi

            # Count lines
            line_count=$(wc -l < "$file" 2>/dev/null || echo "0")

            if [[ $line_count -gt 500 ]]; then
              echo "Large file detected: $file ($line_count lines)"
              if [[ -z "$large_files" ]]; then
                large_files="$file:$line_count"
              else
                large_files="$large_files|$file:$line_count"
              fi
              ((file_count++)) || true
            fi
          done <<< "$changed_files"

          echo "large_files=$large_files" >> "$GITHUB_OUTPUT"
          echo "file_count=$file_count" >> "$GITHUB_OUTPUT"

          if [[ $file_count -gt 0 ]]; then
            echo "Found $file_count large files"
          else
            echo "No large files found"
          fi

      - name: Create Individual Issues for Large Files
        if: steps.find-large.outputs.file_count > 0
        uses: actions/github-script@v7
        with:
          script: |
            const largeFiles = '${{ steps.find-large.outputs.large_files }}';
            const fileCount = '${{ steps.find-large.outputs.file_count }}';

            if (!largeFiles) return;

            // Parse the large files data
            const files = largeFiles.split('|').map(entry => {
              const [path, lines] = entry.split(':');
              return { path, lines: parseInt(lines) };
            });

            console.log(`Processing ${files.length} large files for individual issues`);

            // Get existing issues to avoid duplicates
            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'large-file',
              per_page: 100
            });

            // Extract file paths from existing issue titles
            const existingFilePaths = new Set();
            for (const issue of existingIssues.data) {
              // Match the exact pattern: "Refactor large file: `filepath` (N lines)"
              const match = issue.title.match(/^Refactor large file: `([^`]+)` \(\d+ lines\)$/);
              if (match) {
                existingFilePaths.add(match[1]);
              }
            }

            console.log(`Found ${existingFilePaths.size} existing large file issues:`, Array.from(existingFilePaths));

            let createdCount = 0;
            let skippedCount = 0;

            for (const file of files) {
              // Check if issue already exists for this file
              if (existingFilePaths.has(file.path)) {
                console.log(`Skipping ${file.path} - issue already exists`);
                skippedCount++;
                continue;
              }

              // Determine severity and priority
              const percentage = Math.round((file.lines / 500) * 100);
              let severity, priority, urgency;

              if (file.lines > 1000) {
                severity = 'CRITICAL';
                priority = 'high';
                urgency = '[CRITICAL] High Priority';
              } else if (file.lines > 600) {
                severity = 'WARNING';
                priority = 'medium';
                urgency = '[WARNING] Medium Priority';
              } else {
                severity = 'LARGE';
                priority = 'low';
                urgency = '[LARGE] Normal Priority';
              }

              // Create issue title and body
              const issueTitle = `Refactor large file: \`${file.path}\` (${file.lines} lines)`;

              const issueBody = `## ${urgency} Large File Refactoring Required

            **File**: \`${file.path}\`
            **Current Size**: ${file.lines} lines (${percentage}% over 500-line limit)
            **Severity**: ${severity}
            **Priority**: ${priority}

            ### [STATS] Size Analysis
            - **Target**: < 500 lines
            - **Current**: ${file.lines} lines
            - **Reduction needed**: ${file.lines - 500} lines (${Math.round(((file.lines - 500) / file.lines) * 100)}%)

            ### [REFACTOR] Refactoring Strategies
            ${file.path.endsWith('.py') ? `
            **For Python files:**
            - Extract classes into separate modules
            - Move utility functions to dedicated files
            - Split large functions into smaller, focused ones
            - Move configuration/constants to config files
            - Extract test fixtures and mock data` : ''}
            ${file.path.endsWith('.yml') || file.path.endsWith('.yaml') ? `
            **For YAML files:**
            - Split workflow into multiple files
            - Extract reusable action steps
            - Move configuration to separate files
            - Use matrix strategies for repeated jobs` : ''}
            ${file.path.endsWith('.md') ? `
            **For Markdown files:**
            - Split into multiple focused documents
            - Move sections to dedicated files
            - Create table of contents with links
            - Extract code examples to separate files` : ''}

            ### [PASS] Acceptance Criteria
            - [ ] File reduced to < 500 lines
            - [ ] Functionality preserved (all tests pass)
            - [ ] Clear separation of concerns
            - [ ] No breaking changes to public APIs
            - [ ] Documentation updated if needed

            ### [TARGET] Implementation Notes
            ${file.path.includes('/containers/') ? `
            **Container Context**: This file is part of the containerized microservices architecture.
            - Follow FastAPI-native patterns from agent instructions
            - Maintain standardized API response formats
            - Preserve backward compatibility for legacy endpoints
            - Apply ContentRanker refactoring template pattern` : ''}

            ---
            *Auto-created by Large File Detector workflow*
            *Commit: ${context.sha.substring(0, 7)} | Date: ${new Date().toISOString().split('T')[0]}*`;

              // Create the issue
              try {
                await github.rest.issues.create({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  title: issueTitle,
                  body: issueBody,
                  labels: ['large-file', 'code-quality', 'technical-debt', priority]
                });

                console.log(`Created issue for ${file.path}`);
                createdCount++;
              } catch (error) {
                console.error(`Failed to create issue for ${file.path}:`, error.message);
              }
            }

            console.log(`Summary: Created ${createdCount} issues, Skipped ${skippedCount} existing issues`);

            // Create summary issue if we created multiple individual issues
            if (createdCount > 1) {
              // Check if a recent sprint summary already exists (within last 7 days)
              const recentSummaries = existingIssues.data.filter(issue =>
                issue.title.includes('Large File Refactoring Sprint:') &&
                new Date(issue.created_at) > new Date(Date.now() - 7 * 24 * 60 * 60 * 1000)
              );

              if (recentSummaries.length === 0) {
                const summaryTitle = `Large File Refactoring Sprint: ${createdCount} files need attention`;
                const summaryBody = `## [STATS] Large File Refactoring Sprint Summary

            **Created**: ${new Date().toISOString().split('T')[0]}
            **Files identified**: ${files.length}
            **New issues created**: ${createdCount}
            **Already tracked**: ${skippedCount}

            ### [TARGET] Sprint Overview
            This sprint tracks the systematic refactoring of ${createdCount} large files to improve code maintainability and follow the project's architectural guidelines.

            ### [LIST] Files by Priority

            **[CRITICAL] Critical (>1000 lines):**
            ${files.filter(f => f.lines > 1000).map(f => `- \`${f.path}\` (${f.lines} lines)`).join('\n') || 'None'}

            **[WARNING] High (600-1000 lines):**
            ${files.filter(f => f.lines > 600 && f.lines <= 1000).map(f => `- \`${f.path}\` (${f.lines} lines)`).join('\n') || 'None'}

            **[LARGE] Normal (500-600 lines):**
            ${files.filter(f => f.lines > 500 && f.lines <= 600).map(f => `- \`${f.path}\` (${f.lines} lines)`).join('\n') || 'None'}

            ### [PLAN] Getting Started
            1. Start with Critical priority files (>1000 lines)
            2. Follow the ContentRanker refactoring pattern from recent work
            3. Use FastAPI-native patterns for container services
            4. Maintain backward compatibility and test coverage

            ### [PROGRESS] Progress Tracking
            Individual issues have been created for each file. Close them as refactoring is completed.

            ---
            *Auto-created by Large File Detector workflow*`;

              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: summaryTitle,
                body: summaryBody,
                labels: ['large-file', 'code-quality', 'technical-debt', 'epic']
              });

              console.log('Created sprint summary issue');
              } else {
                console.log(`Skipping sprint summary - found ${recentSummaries.length} recent sprint summaries`);
              }
            }

      - name: Comment on PR
        if: github.event_name == 'pull_request' && steps.find-large.outputs.file_count > 0
        uses: actions/github-script@v7
        with:
          script: |
            const largeFiles = '${{ steps.find-large.outputs.large_files }}';
            const fileCount = '${{ steps.find-large.outputs.file_count }}';

            if (!largeFiles) return;

            const files = largeFiles.split('|').map(entry => {
              const [path, lines] = entry.split(':');
              return { path, lines: parseInt(lines) };
            });

            let comment = `## [SIZE] Large File Detection\n\n`;
            comment += `This PR contains ${fileCount} file${fileCount > 1 ? 's' : ''} over 500 lines:\n\n`;

            files.forEach(file => {
              const severity = file.lines > 1000 ? '[CRITICAL]' : file.lines > 600 ? '[WARNING]' : '[LARGE]';
              comment += `${severity} \`${file.path}\` - ${file.lines} lines\n`;
            });

            comment += `\nConsider refactoring large files for better maintainability. An issue has been created to track this.`;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.number,
              body: comment
            });
