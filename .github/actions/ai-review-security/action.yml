name: 'AI Security Review'
description: 'AI-powered security review from security perspective'
inputs:
  openai-api-key:
    description: 'OpenAI API key for AI analysis (optional - leave empty to disable)'
    required: false
  github-token:
    description: 'GitHub token for commenting on PRs'
    required: true
  model:
    description: 'OpenAI model to use for analysis'
    required: false
    default: 'gpt-4'
  comment-on-pr:
    description: 'Comment review results on PR'
    required: false
    default: 'true'

outputs:
  security-score:
    description: 'Security risk score (1-10)'
    value: ${{ steps.review.outputs.security-score }}
  recommendations:
    description: 'Security recommendations (JSON)'
    value: ${{ steps.review.outputs.recommendations }}

runs:
  using: 'composite'
  steps:
    - name: Prepare Security Review Context
      shell: bash
      run: |
        echo "Preparing security review context..."
        mkdir -p ai-reviews/security

        # Collect security-relevant file changes
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          echo "Collecting changed files for security review..."

          # Get list of changed files
          git diff --name-only origin/${{ github.base_ref }}...HEAD > ai-reviews/security/changed-files.txt

          # Extract security-relevant changes
          git diff origin/${{ github.base_ref }}...HEAD \
            --unified=5 \
            -- '*.py' '*.js' '*.ts' '*.tf' '*.yml' '*.yaml' '*.json' '*.sh' \
            > ai-reviews/security/security-changes.diff
        else
          echo "Collecting full codebase for security review..."
          find . -type f \( -name "*.py" -o -name "*.js" -o -name "*.ts" -o -name "*.tf" -o -name "*.yml" -o -name "*.yaml" -o -name "*.json" -o -name "*.sh" \) \
            -not -path "./.git/*" \
            -not -path "./node_modules/*" \
            -not -path "./__pycache__/*" \
            > ai-reviews/security/all-files.txt
        fi

    - name: Collect Security Scan Results
      shell: bash
      run: |
        echo "Collecting security scan results for AI analysis..."

        # Collect security artifacts if they exist
        if [ -d "security-results" ]; then
          cp -r security-results/* ai-reviews/security/ 2>/dev/null || true
        fi

        # Create security context summary
        review_timestamp="$(date -u +"%Y-%m-%d %H:%M:%S UTC")"
        
        # Check data availability
        if [ -d "security-results" ]; then
          scan_results_status="Available"
        else
          scan_results_status="Not available"
        fi
        
        if [ -f "ai-reviews/security/security-changes.diff" ]; then
          code_changes_status="Available"
        else
          code_changes_status="Full codebase review"
        fi
        
        if [ -f "output/sbom-combined.json" ]; then
          sbom_status="Available"
        else
          sbom_status="Not available"
        fi
        
        cat > ai-reviews/security/context.md << EOF
        # Security Review Context

        **Review Type**: ${{ github.event_name }}
        **Repository**: ${{ github.repository }}
        **Branch**: ${{ github.ref }}
        **Commit**: ${{ github.sha }}
        **Date**: $review_timestamp

        ## Available Security Data
        - Security scan results: $scan_results_status
        - Code changes: $code_changes_status
        - SBOM data: $sbom_status

        ## Focus Areas for Review
        1. Authentication and authorization
        2. Input validation and sanitization
        3. Cryptographic implementations
        4. Infrastructure security configuration
        5. Dependency vulnerabilities
        6. Secrets management
        7. API security
        8. Container security
        EOF

    - name: Run AI Security Review
      id: review
      shell: bash
      env:
        OPENAI_API_KEY: ${{ inputs.openai-api-key }}
      run: |
        echo "ðŸ¤– Checking AI security review configuration..."

        # Check if OpenAI API key is provided
        if [ -z "${{ inputs.openai-api-key }}" ]; then
          echo "OpenAI API key not provided - AI review disabled"
          echo "Using Copilot PR reviews instead for AI-powered analysis"

          # Create placeholder results
          mkdir -p ai-reviews/security
          cat > ai-reviews/security/security-analysis.json << EOF
        {
          "security_score": 5,
          "risk_level": "MEDIUM",
          "critical_findings": [],
          "medium_findings": [],
          "best_practices": ["Use GitHub Copilot PR reviews for AI-powered security analysis"],
          "summary": "AI security review disabled - using Copilot PR reviews for AI analysis"
        }
        EOF

          echo "security-score=5" >> $GITHUB_OUTPUT
          echo "recommendations={\"summary\": \"AI review disabled - using Copilot PR reviews\"}" >> $GITHUB_OUTPUT
          exit 0
        fi

        # Create Python script for AI analysis
        cat > ai-reviews/security/security_review.py << 'EOF'
        import os
        import json
        import requests
        import sys
        from datetime import datetime

        def read_file_safe(filepath):
            """Safely read file content with encoding handling"""
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    return f.read()
            except Exception as e:
                return f"Error reading {filepath}: {str(e)}"

        def analyze_security(changes_content, scan_results, context):
            """Perform AI security analysis"""

            system_prompt = """You are a senior security engineer conducting a comprehensive security review.
            Analyze the provided code changes, security scan results, and context to identify security risks and provide actionable recommendations.

            Focus on:
            1. Authentication/Authorization flaws
            2. Input validation issues
            3. Cryptographic weaknesses
            4. Infrastructure misconfigurations
            5. Dependency vulnerabilities
            6. Secrets exposure
            7. API security issues
            8. Container security concerns

            Provide:
            1. A security risk score (1-10, where 10 is highest risk)
            2. Critical findings that must be addressed
            3. Medium priority improvements
            4. Best practice recommendations
            5. Specific code locations requiring attention

            Be specific, actionable, and focus on practical security improvements."""

            user_content = f"""
            # Security Review Request

            ## Context
            {context}

            ## Code Changes
            {changes_content[:8000]}  # Limit to avoid token limits

            ## Security Scan Results
            {scan_results[:4000]}  # Limit to avoid token limits

            Please provide a comprehensive security analysis in JSON format:
            {{
                "security_score": <1-10>,
                "risk_level": "<LOW|MEDIUM|HIGH|CRITICAL>",
                "critical_findings": [
                    {{
                        "title": "Finding title",
                        "description": "Detailed description",
                        "location": "File/line reference",
                        "impact": "Security impact",
                        "recommendation": "Specific fix"
                    }}
                ],
                "medium_findings": [
                    {{
                        "title": "Finding title",
                        "description": "Detailed description",
                        "location": "File/line reference",
                        "recommendation": "Improvement suggestion"
                    }}
                ],
                "best_practices": [
                    "Recommendation 1",
                    "Recommendation 2"
                ],
                "summary": "Overall security assessment summary"
            }}
            """

            try:
                response = requests.post(
                    'https://api.openai.com/v1/chat/completions',
                    headers={
                        'Authorization': f'Bearer {os.environ["OPENAI_API_KEY"]}',
                        'Content-Type': 'application/json'
                    },
                    json={
                        'model': 'gpt-4',
                        'messages': [
                            {'role': 'system', 'content': system_prompt},
                            {'role': 'user', 'content': user_content}
                        ],
                        'temperature': 0.3,
                        'max_tokens': 2000
                    }
                )

                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']

                    # Try to extract JSON from response
                    try:
                        # Find JSON block in response
                        start = content.find('{')
                        end = content.rfind('}') + 1
                        if start >= 0 and end > start:
                            return json.loads(content[start:end])
                    except json.JSONDecodeError:
                        pass

                    # Fallback if JSON parsing fails
                    return {
                        "security_score": 5,
                        "risk_level": "MEDIUM",
                        "critical_findings": [],
                        "medium_findings": [],
                        "best_practices": ["Review AI analysis output manually"],
                        "summary": f"AI analysis completed but JSON parsing failed. Raw content: {content[:500]}..."
                    }
                else:
                    return {
                        "security_score": 5,
                        "risk_level": "MEDIUM",
                        "critical_findings": [],
                        "medium_findings": [],
                        "best_practices": ["Manual security review recommended"],
                        "summary": f"AI analysis failed with status {response.status_code}"
                    }

            except Exception as e:
                return {
                    "security_score": 5,
                    "risk_level": "MEDIUM",
                    "critical_findings": [],
                    "medium_findings": [],
                    "best_practices": ["Manual security review recommended"],
                    "summary": f"AI analysis error: {str(e)}"
                }

        # Main execution
        if __name__ == "__main__":
            # Read inputs
            context = read_file_safe('context.md')

            changes_content = ""
            if os.path.exists('security-changes.diff'):
                changes_content = read_file_safe('security-changes.diff')
            elif os.path.exists('all-files.txt'):
                # For full codebase review, read key files
                with open('all-files.txt', 'r') as f:
                    files = f.read().strip().split('\n')
                for file in files[:10]:  # Limit files to avoid token limits
                    if os.path.exists(file):
                        changes_content += f"\n\n=== {file} ===\n"
                        changes_content += read_file_safe(file)[:2000]  # Limit per file

            # Collect scan results
            scan_results = ""
            for result_file in ['trivy-results.json', 'semgrep-results.json', 'safety-results.json']:
                if os.path.exists(result_file):
                    scan_results += f"\n\n=== {result_file} ===\n"
                    scan_results += read_file_safe(result_file)[:1000]

            # Perform analysis
            analysis = analyze_security(changes_content, scan_results, context)

            # Save results
            with open('security-analysis.json', 'w') as f:
                json.dump(analysis, f, indent=2)

            # Output for GitHub Actions
            print(f"security-score={analysis.get('security_score', 5)}")
            print(f"recommendations={json.dumps(analysis)}")
        EOF

        # Run the security analysis
        cd ai-reviews/security
        python security_review.py > review-output.txt 2>&1

        # Extract outputs
        if [ -f "review-output.txt" ]; then
          security_score="$(grep "security-score=" review-output.txt | cut -d'=' -f2)"
          recommendations="$(grep "recommendations=" review-output.txt | cut -d'=' -f2-)"

          echo "security-score=${security_score:-5}" >> "$GITHUB_OUTPUT"
          echo "recommendations=${recommendations:-{}}" >> "$GITHUB_OUTPUT"
        else
          echo "security-score=5" >> "$GITHUB_OUTPUT"
          echo "recommendations={}" >> "$GITHUB_OUTPUT"
        fi

    - name: Generate Security Report
      shell: bash
      run: |
        echo "ðŸ“„ Generating security review report..."

        cd ai-reviews/security

        # Create markdown report
        report_timestamp="$(date -u +"%Y-%m-%d %H:%M:%S UTC")"
        
        # Extract security data
        if [ -f "security-analysis.json" ]; then
          security_score="$(jq -r '.security_score // "N/A"' security-analysis.json)"
          risk_level="$(jq -r '.risk_level // "UNKNOWN"' security-analysis.json)"
          critical_findings="$(jq -r '.critical_findings[]? | "- **\(.title)**: \(.description) (Location: \(.location))"' security-analysis.json)" || critical_findings="None identified"
          medium_findings="$(jq -r '.medium_findings[]? | "- **\(.title)**: \(.description) (Location: \(.location))"' security-analysis.json)" || medium_findings="None identified"
          best_practices="$(jq -r '.best_practices[]? | "- \(.)"' security-analysis.json)" || best_practices="Manual review recommended"
          summary="$(jq -r '.summary // "Security analysis completed"' security-analysis.json)"
        else
          security_score="N/A"
          risk_level="UNKNOWN"
          critical_findings="None identified"
          medium_findings="None identified"
          best_practices="Manual review recommended"
          summary="Security analysis completed"
        fi
        
        cat > security-review.md << EOF
        # AI Security Review Report

        **Analysis Date**: $report_timestamp
        **Security Score**: $security_score
        **Risk Level**: $risk_level

        ## Critical Findings
        $critical_findings

        ## Medium Priority Findings
        $medium_findings

        ## Security Recommendations
        $best_practices

        ## Summary
        $summary

        ---
        *Generated by AI Security Review Assistant*
        EOF

    - name: Comment on PR
      if: github.event_name == 'pull_request' && inputs.comment-on-pr == 'true'
      uses: actions/github-script@v7
      env:
        GITHUB_TOKEN: ${{ inputs.github-token }}
      with:
        script: |
          const fs = require('fs');

          let report = "Security review completed.";
          try {
            report = fs.readFileSync('ai-reviews/security/security-review.md', 'utf8');
          } catch (error) {
            console.log('Could not read security report:', error.message);
          }

          const comment = `${report}`;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: Upload Security Review Artifacts
      uses: actions/upload-artifact@v4
      with:
        name: ai-security-review
        path: |
          ai-reviews/security/
        retention-days: 30
